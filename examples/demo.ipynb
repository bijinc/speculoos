{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "\n",
    "def sample_random(p: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Samples a token index from the given probability distribution p.\"\"\"\n",
    "    return torch.multinomial(p, num_samples=1)\n",
    "\n",
    "def relu_n(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the ReLU function and applies normalization\"\"\"\n",
    "    x_relu = torch.relu(x)\n",
    "    return x_relu / torch.sum(x_relu)\n",
    "\n",
    "def predict(model: AutoModelForCausalLM, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Gets the probability distribution over the next token given the input sequence x using the model.\n",
    "        - model: the language model to use for prediction\n",
    "        - x: the input sequence (token IDs)\n",
    "    Returns the probability distribution over the next token.\n",
    "    \"\"\"\n",
    "    # Get logits from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Sample next token from the last position\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    return torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "def auto_regressive_sampling(model: AutoModelForCausalLM, x: torch.Tensor, T: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Auto-regressive sampling from the target model for T steps.\n",
    "        - model: the target language model to sample from\n",
    "        - x: the initial input sequence (token IDs)\n",
    "        - T: the number of tokens to sample\n",
    "    Returns the generated sequence of token IDs after T steps.\n",
    "    \"\"\"\n",
    "    for _ in range(T):\n",
    "        # get probabilities\n",
    "        probabilities = predict(model, x)\n",
    "\n",
    "        # sample next token from the target model\n",
    "        predicted_token = sample_random(probabilities)\n",
    "        \n",
    "        # Concatenate the new token\n",
    "        x = torch.cat([x, predicted_token], dim=-1)\n",
    "\n",
    "    return x\n",
    "\n",
    "def speculative_sampling(\n",
    "        target_model: AutoModelForCausalLM,\n",
    "        draft_model: AutoModelForCausalLM,\n",
    "        x: torch.Tensor,\n",
    "        K: int,\n",
    "        T: int,\n",
    "        eps=1e-10\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Speculative sampling from the target model using a draft model for K steps.\n",
    "        - target_model: the target language model to sample from\n",
    "        - draft_model: the draft language model used for proposing tokens\n",
    "        - x: the initial input sequence (token IDs)\n",
    "        - K: the number of speculative steps to perform\n",
    "        - T: the total number of tokens to sample (including speculative steps)\n",
    "    Returns the generated sequence of token IDs after T steps.\n",
    "    \"\"\"\n",
    "    n = x.shape[-1]\n",
    "    T += n\n",
    "\n",
    "    while n < T:\n",
    "        # Save the starting position for this iteration\n",
    "        n_start = n\n",
    "        \n",
    "        # Drafting: auto-regressive sampling using the draft model\n",
    "        x_draft = x.clone()\n",
    "        draft_probs = []\n",
    "        \n",
    "        for _ in range(K):\n",
    "            # get probabilities\n",
    "            p = predict(draft_model, x_draft)\n",
    "            draft_probs.append(p)\n",
    "            \n",
    "            # sample next token from the draft model\n",
    "            predicted_token = sample_random(p)\n",
    "            \n",
    "            # Concatenate the new token\n",
    "            x_draft = torch.cat([x_draft, predicted_token], dim=-1)\n",
    "        \n",
    "        # Verification: get target model probabilities for entire draft sequence\n",
    "        with torch.no_grad():\n",
    "            outputs = target_model(x_draft)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        target_probs = []\n",
    "        for k in range(K + 1):\n",
    "            pos = n_start - 1 + k\n",
    "            next_token_logits = logits[:, pos, :]\n",
    "            target_probs.append(torch.softmax(next_token_logits, dim=-1))\n",
    "\n",
    "        # Correction: accept or reject predicted tokens\n",
    "        all_accepted = True\n",
    "        for k in range(K):\n",
    "            j = x_draft[:, n_start + k]  # Token at position n_start+k\n",
    "            \n",
    "            p_j = draft_probs[k][0, j.item()]  # Draft probability for token j\n",
    "            q_j = target_probs[k][0, j.item()]  # Target probability for token j\n",
    "\n",
    "            r = random.random()\n",
    "            if r < min(1.0, (q_j / (p_j + eps)).item()):\n",
    "                # token accepted\n",
    "                x = torch.cat([x, j.unsqueeze(0)], dim=-1)\n",
    "                n += 1\n",
    "            else:\n",
    "                # token rejected, resample\n",
    "                adjusted_probs = relu_n(target_probs[k][0] - draft_probs[k][0])\n",
    "                resampled_token = sample_random(adjusted_probs.unsqueeze(0))\n",
    "                x = torch.cat([x, resampled_token], dim=-1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "        \n",
    "        if all_accepted:\n",
    "            # sample an extra token from target model at the last position\n",
    "            x = torch.cat([x, sample_random(target_probs[-1])], dim=-1)\n",
    "            n += 1\n",
    "    \n",
    "    return x\n",
    "\n",
    "def decode(output_ids: torch.Tensor, tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Decodes the generated token IDs into text using the model's tokenizer.\n",
    "        - output_ids: the sequence of generated token IDs\n",
    "        - tokenizer: the tokenizer used for decoding\n",
    "    Returns the decoded text string.\n",
    "    \"\"\"\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"Once upon a time\"\n",
    "model_name = \"gpt2-medium\"\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(input_seq, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"\\nRunning auto-regressive sampling using {model_name}\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "output_ids = auto_regressive_sampling(model, input_ids, T=20)\n",
    "text = decode(output_ids, tokenizer)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Output: {text}\")\n",
    "print(f\"Time: {elapsed_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speculative sampling\n",
    "draft_model_name = \"gpt2\"\n",
    "target_model_name = \"gpt2-medium\"\n",
    "input_seq = \"Once upon a time\"\n",
    "\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(input_seq, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"\\nRunning speculative sampling using {target_model_name} with draft model {draft_model_name}\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "output_ids = speculative_sampling(target_model, draft_model, input_ids, K=5, T=20)\n",
    "text = decode(output_ids, tokenizer)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Output: {text}\")\n",
    "print(f\"Time: {elapsed_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speculoos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

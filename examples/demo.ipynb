{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install torch transformers"
      ],
      "metadata": {
        "id": "fdDYhaJo_UsH",
        "outputId": "5b700a64-0e87-4a8b-deb7-0a80efc6b9df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fdDYhaJo_UsH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "95aa5422",
      "metadata": {
        "id": "95aa5422"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import random\n",
        "import time\n",
        "\n",
        "def sample_random(p: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Samples a token index from the given probability distribution p.\"\"\"\n",
        "    return torch.multinomial(p, num_samples=1)\n",
        "\n",
        "def relu_n(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Computes the ReLU function and applies normalization\"\"\"\n",
        "    x_relu = torch.relu(x)\n",
        "    return x_relu / torch.sum(x_relu)\n",
        "\n",
        "def predict(model: AutoModelForCausalLM, x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Gets the probability distribution over the next token given the input sequence x using the model.\n",
        "        - model: the language model to use for prediction\n",
        "        - x: the input sequence (token IDs)\n",
        "    Returns the probability distribution over the next token.\n",
        "    \"\"\"\n",
        "    # Get logits from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Sample next token from the last position\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    return torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "def auto_regressive_sampling(model: AutoModelForCausalLM, x: torch.Tensor, T: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Auto-regressive sampling from the target model for T steps.\n",
        "        - model: the target language model to sample from\n",
        "        - x: the initial input sequence (token IDs)\n",
        "        - T: the number of tokens to sample\n",
        "    Returns the generated sequence of token IDs after T steps.\n",
        "    \"\"\"\n",
        "    for _ in range(T):\n",
        "        # get probabilities\n",
        "        probabilities = predict(model, x)\n",
        "\n",
        "        # sample next token from the target model\n",
        "        predicted_token = sample_random(probabilities)\n",
        "\n",
        "        # Concatenate the new token\n",
        "        x = torch.cat([x, predicted_token], dim=-1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def speculative_sampling(\n",
        "        target_model: AutoModelForCausalLM,\n",
        "        draft_model: AutoModelForCausalLM,\n",
        "        x: torch.Tensor,\n",
        "        K: int,\n",
        "        T: int,\n",
        "        eps=1e-10\n",
        "    ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Speculative sampling from the target model using a draft model for K steps.\n",
        "        - target_model: the target language model to sample from\n",
        "        - draft_model: the draft language model used for proposing tokens\n",
        "        - x: the initial input sequence (token IDs)\n",
        "        - K: the number of speculative steps to perform\n",
        "        - T: the total number of tokens to sample (including speculative steps)\n",
        "    Returns the generated sequence of token IDs after T steps.\n",
        "    \"\"\"\n",
        "    n = x.shape[-1]\n",
        "    T += n\n",
        "\n",
        "    while n < T:\n",
        "        # Save the starting position for this iteration\n",
        "        n_start = n\n",
        "\n",
        "        # Drafting: auto-regressive sampling using the draft model\n",
        "        x_draft = x.clone()\n",
        "        draft_probs = []\n",
        "\n",
        "        for _ in range(K):\n",
        "            # get probabilities\n",
        "            p = predict(draft_model, x_draft)\n",
        "            draft_probs.append(p)\n",
        "\n",
        "            # sample next token from the draft model\n",
        "            predicted_token = sample_random(p)\n",
        "\n",
        "            # Concatenate the new token\n",
        "            x_draft = torch.cat([x_draft, predicted_token], dim=-1)\n",
        "\n",
        "        # Verification: get target model probabilities for entire draft sequence\n",
        "        with torch.no_grad():\n",
        "            outputs = target_model(x_draft)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        target_probs = []\n",
        "        for k in range(K + 1):\n",
        "            pos = n_start - 1 + k\n",
        "            next_token_logits = logits[:, pos, :]\n",
        "            target_probs.append(torch.softmax(next_token_logits, dim=-1))\n",
        "\n",
        "        # Correction: accept or reject predicted tokens\n",
        "        all_accepted = True\n",
        "        for k in range(K):\n",
        "            j = x_draft[:, n_start + k]  # Token at position n_start+k\n",
        "\n",
        "            p_j = draft_probs[k][0, j.item()]  # Draft probability for token j\n",
        "            q_j = target_probs[k][0, j.item()]  # Target probability for token j\n",
        "\n",
        "            r = random.random()\n",
        "            if r < min(1.0, (q_j / (p_j + eps)).item()):\n",
        "                # token accepted\n",
        "                x = torch.cat([x, j.unsqueeze(0)], dim=-1)\n",
        "                n += 1\n",
        "            else:\n",
        "                # token rejected, resample\n",
        "                adjusted_probs = relu_n(target_probs[k][0] - draft_probs[k][0])\n",
        "                resampled_token = sample_random(adjusted_probs.unsqueeze(0))\n",
        "                x = torch.cat([x, resampled_token], dim=-1)\n",
        "                n += 1\n",
        "                all_accepted = False\n",
        "                break\n",
        "\n",
        "        if all_accepted:\n",
        "            # sample an extra token from target model at the last position\n",
        "            x = torch.cat([x, sample_random(target_probs[-1])], dim=-1)\n",
        "            n += 1\n",
        "\n",
        "    return x\n",
        "\n",
        "def decode(output_ids: torch.Tensor, tokenizer: AutoTokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Decodes the generated token IDs into text using the model's tokenizer.\n",
        "        - output_ids: the sequence of generated token IDs\n",
        "        - tokenizer: the tokenizer used for decoding\n",
        "    Returns the decoded text string.\n",
        "    \"\"\"\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def speculative_sampling_batched(\n",
        "        target_model: AutoModelForCausalLM,\n",
        "        draft_model: AutoModelForCausalLM,\n",
        "        x: torch.Tensor,\n",
        "        K: int,\n",
        "        T: int,\n",
        "        eps=1e-10\n",
        "    ) -> torch.Tensor:\n",
        "    \"\"\"Optimized speculative sampling with batched draft generation.\"\"\"\n",
        "    n = x.shape[-1]\n",
        "    T += n\n",
        "    device = x.device\n",
        "\n",
        "    while n < T:\n",
        "        n_start = n\n",
        "\n",
        "        # Drafting: Generate K tokens in a more efficient way\n",
        "        x_draft = x.clone()\n",
        "        draft_probs = []\n",
        "\n",
        "        # Still need sequential generation but minimize overhead\n",
        "        with torch.no_grad():\n",
        "            for _ in range(K):\n",
        "                outputs = draft_model(x_draft)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "                p = torch.softmax(logits, dim=-1)\n",
        "                draft_probs.append(p)\n",
        "\n",
        "                predicted_token = torch.multinomial(p, num_samples=1)\n",
        "                x_draft = torch.cat([x_draft, predicted_token], dim=-1)\n",
        "\n",
        "        # Verification: Single forward pass for all K tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = target_model(x_draft)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Compute all target probs at once\n",
        "        target_probs = torch.softmax(logits[:, n_start-1:n_start+K, :], dim=-1)\n",
        "\n",
        "        # Correction: vectorized acceptance check\n",
        "        all_accepted = True\n",
        "        for k in range(K):\n",
        "            j = x_draft[:, n_start + k]\n",
        "\n",
        "            p_j = draft_probs[k][0, j.item()]\n",
        "            q_j = target_probs[0, k, j.item()]\n",
        "\n",
        "            r = torch.rand(1, device=device).item()\n",
        "            if r < min(1.0, (q_j / (p_j + eps)).item()):\n",
        "                x = torch.cat([x, j.unsqueeze(0)], dim=-1)\n",
        "                n += 1\n",
        "            else:\n",
        "                adjusted_probs = relu_n(target_probs[0, k] - draft_probs[k][0])\n",
        "                resampled_token = sample_random(adjusted_probs.unsqueeze(0))\n",
        "                x = torch.cat([x, resampled_token], dim=-1)\n",
        "                n += 1\n",
        "                all_accepted = False\n",
        "                break\n",
        "\n",
        "        if all_accepted:\n",
        "            bonus_probs = torch.softmax(logits[:, n_start + K - 1, :], dim=-1)\n",
        "            x = torch.cat([x, sample_random(bonus_probs)], dim=-1)\n",
        "            n += 1\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Ny2GYuq6DIB_"
      },
      "id": "Ny2GYuq6DIB_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "D5M9lZB-AEks",
        "outputId": "efda2acb-d698-44c7-a940-4d367a024815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "D5M9lZB-AEks",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "769f0172",
      "metadata": {
        "id": "769f0172",
        "outputId": "635bb18c-e304-4ff0-a885-d4502e6f0fa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "9f37360282de4e7ba477d0a12ace2a6a",
            "c3f07a6720a547cf8b2f16969a264d05",
            "41a7d113eab147f1b1eaf365c1c66247",
            "1d6445436ec64493b29722bf875cbd02",
            "b6f7500c6d734ed1b6c140aa789baaa7",
            "50c05c51be7444d39c27f746d358353a",
            "28b0cd512ab94f08bd673f94bb6c290a",
            "1787de9c994d41e68f229357aaeb3610",
            "ba5cc76db90d423e8d5e50ca91c50514",
            "628654de4f26485d95289b48b11b5630",
            "5b047f53145a41dd96bbe308d0aca552"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/436 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f37360282de4e7ba477d0a12ace2a6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2-large\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...35}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running auto-regressive sampling using gpt2-large\n",
            "Output: Once upon a time the programme belonged to Company I, the Indian Infantry Guards, who lost all means of official adaptation. Efforts were made to back it up in light of scientific knowledge, knowledge acquired by mail from foreign officers, published publications in French, German and English, modern legislation with local regulations, American propensities keeping Group Kempe out of plain sight, and eight Christian Brothers fighting numerically the number of fifty-eight men required to have any chance of action, not counting the twelve National Guardsmen expected\n",
            "Time: 2.26s\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"gpt2-large\"\n",
        "target_model = torch.compile(AutoModelForCausalLM.from_pretrained(target_model_name).to(device).eval())\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_name)\n",
        "\n",
        "input_seq = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_seq, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"\\nRunning auto-regressive sampling using {target_model_name}\")\n",
        "start = time.perf_counter()\n",
        "\n",
        "output_ids = auto_regressive_sampling(target_model, input_ids, T=100)\n",
        "text = decode(output_ids, tokenizer)\n",
        "\n",
        "elapsed_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"Output: {text}\")\n",
        "print(f\"Time: {elapsed_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "494f2048",
      "metadata": {
        "id": "494f2048",
        "outputId": "e20102f2-05a5-490c-86e4-37ace6da556c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "8fe856306a66496c86b60707a8406b29",
            "1dc4afdc762e4ca9883168f9ab8cba09",
            "ea47d1a2f6444765ba440c97a593e024",
            "c9f60809c23943c984612d697c8243b8",
            "c93b3eda40f14a74ad8e73ba5bd4c6d7",
            "b4fd381393574549a927412faf3922a7",
            "9534c09807b44ccebcbbe1c41a977ac8",
            "61ef267147d0458f8ed9f4fc8d1e6e7f",
            "e8907868ae6d450da7d91eb919230007",
            "8646a0f66fd64b968fc3d2ff3bec22b9",
            "2c7b33aa239b494ebc5590cf3f5c7d02"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fe856306a66496c86b60707a8406b29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running speculative sampling using gpt2-large with draft model gpt2\n",
            "Output: Once upon a time I would have been frozen with fear under the sawdust-choked roadside.\" (See Every minute leads to annihilation.)\n",
            "\n",
            "Living in upstate New York\n",
            "\n",
            "Humans in their Christian-dominated world generally see any type of food, dress, song or dive as forbidden, upon pain of hellfire and eternal damnation.\n",
            "\n",
            "However, on a recent visit to upstate New York (8750 Queen Anne Avenue), my daughter and I had dinner at a ranch owned by a man\n",
            "Time: 1.61s\n"
          ]
        }
      ],
      "source": [
        "# Speculative sampling\n",
        "draft_model_name = \"gpt2\"\n",
        "draft_model = torch.compile(AutoModelForCausalLM.from_pretrained(draft_model_name).to(device).eval())\n",
        "\n",
        "draft_model.config.use_cache = True\n",
        "target_model.config.use_cache = True\n",
        "\n",
        "input_seq = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_seq, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"\\nRunning speculative sampling using {target_model_name} with draft model {draft_model_name}\")\n",
        "start = time.perf_counter()\n",
        "\n",
        "output_ids = speculative_sampling_batched(target_model, draft_model, input_ids, K=5, T=100)\n",
        "text = decode(output_ids, tokenizer)\n",
        "\n",
        "elapsed_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"Output: {text}\")\n",
        "print(f\"Time: {elapsed_time:.2f}s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f37360282de4e7ba477d0a12ace2a6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3f07a6720a547cf8b2f16969a264d05",
              "IPY_MODEL_41a7d113eab147f1b1eaf365c1c66247",
              "IPY_MODEL_1d6445436ec64493b29722bf875cbd02"
            ],
            "layout": "IPY_MODEL_b6f7500c6d734ed1b6c140aa789baaa7"
          }
        },
        "c3f07a6720a547cf8b2f16969a264d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c05c51be7444d39c27f746d358353a",
            "placeholder": "​",
            "style": "IPY_MODEL_28b0cd512ab94f08bd673f94bb6c290a",
            "value": "Loading weights: 100%"
          }
        },
        "41a7d113eab147f1b1eaf365c1c66247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1787de9c994d41e68f229357aaeb3610",
            "max": 436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba5cc76db90d423e8d5e50ca91c50514",
            "value": 436
          }
        },
        "1d6445436ec64493b29722bf875cbd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_628654de4f26485d95289b48b11b5630",
            "placeholder": "​",
            "style": "IPY_MODEL_5b047f53145a41dd96bbe308d0aca552",
            "value": " 436/436 [00:00&lt;00:00, 1351.30it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "b6f7500c6d734ed1b6c140aa789baaa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c05c51be7444d39c27f746d358353a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28b0cd512ab94f08bd673f94bb6c290a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1787de9c994d41e68f229357aaeb3610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba5cc76db90d423e8d5e50ca91c50514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "628654de4f26485d95289b48b11b5630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b047f53145a41dd96bbe308d0aca552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fe856306a66496c86b60707a8406b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dc4afdc762e4ca9883168f9ab8cba09",
              "IPY_MODEL_ea47d1a2f6444765ba440c97a593e024",
              "IPY_MODEL_c9f60809c23943c984612d697c8243b8"
            ],
            "layout": "IPY_MODEL_c93b3eda40f14a74ad8e73ba5bd4c6d7"
          }
        },
        "1dc4afdc762e4ca9883168f9ab8cba09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4fd381393574549a927412faf3922a7",
            "placeholder": "​",
            "style": "IPY_MODEL_9534c09807b44ccebcbbe1c41a977ac8",
            "value": "Loading weights: 100%"
          }
        },
        "ea47d1a2f6444765ba440c97a593e024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61ef267147d0458f8ed9f4fc8d1e6e7f",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8907868ae6d450da7d91eb919230007",
            "value": 148
          }
        },
        "c9f60809c23943c984612d697c8243b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8646a0f66fd64b968fc3d2ff3bec22b9",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7b33aa239b494ebc5590cf3f5c7d02",
            "value": " 148/148 [00:00&lt;00:00, 901.21it/s, Materializing param=transformer.wte.weight]"
          }
        },
        "c93b3eda40f14a74ad8e73ba5bd4c6d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4fd381393574549a927412faf3922a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9534c09807b44ccebcbbe1c41a977ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61ef267147d0458f8ed9f4fc8d1e6e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8907868ae6d450da7d91eb919230007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8646a0f66fd64b968fc3d2ff3bec22b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7b33aa239b494ebc5590cf3f5c7d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}